{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import RobertaTokenizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset etiquetado en un DataFrame\n",
    "df = pd.read_csv(r\"C:\\Users\\samue\\OneDrive\\Escritorio\\df_bipolar_full_sept7_ok.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>submission_type</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Media: Nobody knows what kamala is about\\n\\nMe...</td>\n",
       "      <td>comment</td>\n",
       "      <td>politics</td>\n",
       "      <td>Democrat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NYT breaking news that Netanyahu has agreed to...</td>\n",
       "      <td>comment</td>\n",
       "      <td>politics</td>\n",
       "      <td>Democrat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I was thinking this morning about how freaking...</td>\n",
       "      <td>comment</td>\n",
       "      <td>politics</td>\n",
       "      <td>Democrat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I was on a break talking to a 25-year old cowo...</td>\n",
       "      <td>comment</td>\n",
       "      <td>politics</td>\n",
       "      <td>Republican</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Went to the Kamala rally today in WI and it wa...</td>\n",
       "      <td>comment</td>\n",
       "      <td>politics</td>\n",
       "      <td>Democrat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text submission_type  \\\n",
       "0  Media: Nobody knows what kamala is about\\n\\nMe...         comment   \n",
       "1  NYT breaking news that Netanyahu has agreed to...         comment   \n",
       "2  I was thinking this morning about how freaking...         comment   \n",
       "3  I was on a break talking to a 25-year old cowo...         comment   \n",
       "4  Went to the Kamala rally today in WI and it wa...         comment   \n",
       "\n",
       "  subreddit      labels  \n",
       "0  politics    Democrat  \n",
       "1  politics    Democrat  \n",
       "2  politics    Democrat  \n",
       "3  politics  Republican  \n",
       "4  politics    Democrat  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84045, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapeamos las etiquetas de texto a valores num√©ricos\n",
    "label_mapping = {\"Republican\": 0, \"Democrat\": 1}\n",
    "\n",
    "df['labels'] = df['labels'].map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['labels'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir a Dataset compatible con Hugging Face\n",
    "from datasets import Dataset\n",
    "\n",
    "# Guardar las etiquetas reales antes de hacer el drop\n",
    "true_labels = test_df['labels'].values\n",
    "\n",
    "# Eliminar la columna 'labels' para preparar el test sin las etiquetas\n",
    "test_features = test_df.drop(columns=['labels'])\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\samue\\anaconda3\\envs\\project_g\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64e944037b424fc7979f3387ea0196cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/67236 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "096b4a485d244ee0b9bfc8b9fde6a3a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16809 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cargar el tokenizador de RoBERTa\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Tokenizar los datos\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Tokenizar los conjuntos de entrenamiento y prueba\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\samue\\\\OneDrive\\\\Escritorio\\\\Docs\\\\4GeeksAcademy\\\\FINAL_PROJECT\\\\4geeks_finalproject\\\\src/tokenizer\\\\tokenizer_config.json',\n",
       " 'C:\\\\Users\\\\samue\\\\OneDrive\\\\Escritorio\\\\Docs\\\\4GeeksAcademy\\\\FINAL_PROJECT\\\\4geeks_finalproject\\\\src/tokenizer\\\\special_tokens_map.json',\n",
       " 'C:\\\\Users\\\\samue\\\\OneDrive\\\\Escritorio\\\\Docs\\\\4GeeksAcademy\\\\FINAL_PROJECT\\\\4geeks_finalproject\\\\src/tokenizer\\\\vocab.json',\n",
       " 'C:\\\\Users\\\\samue\\\\OneDrive\\\\Escritorio\\\\Docs\\\\4GeeksAcademy\\\\FINAL_PROJECT\\\\4geeks_finalproject\\\\src/tokenizer\\\\merges.txt',\n",
       " 'C:\\\\Users\\\\samue\\\\OneDrive\\\\Escritorio\\\\Docs\\\\4GeeksAcademy\\\\FINAL_PROJECT\\\\4geeks_finalproject\\\\src/tokenizer\\\\added_tokens.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Guardar el tokenizador utilizado\n",
    "tokenizer.save_pretrained(r\"C:\\Users\\samue\\OneDrive\\Escritorio\\Docs\\4GeeksAcademy\\FINAL_PROJECT\\4geeks_finalproject\\src/tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si tambi√©n guardaste el tokenizador\n",
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(r\"C:\\Users\\samue\\OneDrive\\Escritorio\\Docs\\4GeeksAcademy\\FINAL_PROJECT\\4geeks_finalproject\\src\\tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\samue\\anaconda3\\envs\\project_g\\lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2536136f374f4a10b3a08f9538a18401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25215 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1532, 'grad_norm': 0.06808625906705856, 'learning_rate': 1.9603410668253024e-05, 'epoch': 0.06}\n",
      "{'loss': 0.0693, 'grad_norm': 0.004451404791325331, 'learning_rate': 1.920682133650605e-05, 'epoch': 0.12}\n",
      "{'loss': 0.0481, 'grad_norm': 0.03014403209090233, 'learning_rate': 1.8810232004759074e-05, 'epoch': 0.18}\n",
      "{'loss': 0.038, 'grad_norm': 0.00703075435012579, 'learning_rate': 1.84136426730121e-05, 'epoch': 0.24}\n",
      "{'loss': 0.0321, 'grad_norm': 0.41191479563713074, 'learning_rate': 1.801705334126512e-05, 'epoch': 0.3}\n",
      "{'loss': 0.0308, 'grad_norm': 0.0018506153719499707, 'learning_rate': 1.7620464009518146e-05, 'epoch': 0.36}\n",
      "{'loss': 0.0318, 'grad_norm': 0.3027184009552002, 'learning_rate': 1.722387467777117e-05, 'epoch': 0.42}\n",
      "{'loss': 0.0239, 'grad_norm': 0.0017950630281120539, 'learning_rate': 1.6827285346024194e-05, 'epoch': 0.48}\n",
      "{'loss': 0.0179, 'grad_norm': 0.0008981557912193239, 'learning_rate': 1.6430696014277215e-05, 'epoch': 0.54}\n",
      "{'loss': 0.021, 'grad_norm': 0.0005893719498999417, 'learning_rate': 1.603410668253024e-05, 'epoch': 0.59}\n",
      "{'loss': 0.0187, 'grad_norm': 0.00553162069991231, 'learning_rate': 1.5637517350783266e-05, 'epoch': 0.65}\n",
      "{'loss': 0.0098, 'grad_norm': 0.0012675321195274591, 'learning_rate': 1.524092801903629e-05, 'epoch': 0.71}\n",
      "{'loss': 0.0093, 'grad_norm': 0.00039586835191585124, 'learning_rate': 1.4844338687289313e-05, 'epoch': 0.77}\n",
      "{'loss': 0.0185, 'grad_norm': 39.644371032714844, 'learning_rate': 1.4447749355542338e-05, 'epoch': 0.83}\n",
      "{'loss': 0.0185, 'grad_norm': 0.003366745775565505, 'learning_rate': 1.405116002379536e-05, 'epoch': 0.89}\n",
      "{'loss': 0.0106, 'grad_norm': 0.0003243984829168767, 'learning_rate': 1.3654570692048386e-05, 'epoch': 0.95}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f9eb5993124d5a8625a0ced6a2d76d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 331.1612, 'eval_samples_per_second': 50.758, 'eval_steps_per_second': 6.347, 'epoch': 1.0}\n",
      "{'loss': 0.0173, 'grad_norm': 0.0010047669056802988, 'learning_rate': 1.3257981360301409e-05, 'epoch': 1.01}\n",
      "{'loss': 0.0059, 'grad_norm': 0.0012426926987245679, 'learning_rate': 1.2861392028554434e-05, 'epoch': 1.07}\n",
      "{'loss': 0.0029, 'grad_norm': 0.0002543394803069532, 'learning_rate': 1.2464802696807456e-05, 'epoch': 1.13}\n",
      "{'loss': 0.0063, 'grad_norm': 0.0001983623660635203, 'learning_rate': 1.2068213365060482e-05, 'epoch': 1.19}\n",
      "{'loss': 0.0064, 'grad_norm': 0.0004751285305246711, 'learning_rate': 1.1671624033313505e-05, 'epoch': 1.25}\n",
      "{'loss': 0.0039, 'grad_norm': 0.00017160511924885213, 'learning_rate': 1.127503470156653e-05, 'epoch': 1.31}\n",
      "{'loss': 0.0142, 'grad_norm': 0.000745914876461029, 'learning_rate': 1.0878445369819552e-05, 'epoch': 1.37}\n",
      "{'loss': 0.0088, 'grad_norm': 0.00021610646217595786, 'learning_rate': 1.0481856038072578e-05, 'epoch': 1.43}\n",
      "{'loss': 0.0068, 'grad_norm': 0.00044472189620137215, 'learning_rate': 1.0085266706325601e-05, 'epoch': 1.49}\n",
      "{'loss': 0.0072, 'grad_norm': 0.00027962427702732384, 'learning_rate': 9.688677374578625e-06, 'epoch': 1.55}\n",
      "{'loss': 0.0074, 'grad_norm': 21.473430633544922, 'learning_rate': 9.292088042831648e-06, 'epoch': 1.61}\n",
      "{'loss': 0.0111, 'grad_norm': 0.00046088662929832935, 'learning_rate': 8.895498711084672e-06, 'epoch': 1.67}\n",
      "{'loss': 0.0064, 'grad_norm': 0.0004173085035290569, 'learning_rate': 8.498909379337697e-06, 'epoch': 1.73}\n",
      "{'loss': 0.0081, 'grad_norm': 0.00027997081633657217, 'learning_rate': 8.10232004759072e-06, 'epoch': 1.78}\n",
      "{'loss': 0.0104, 'grad_norm': 0.00030854364740662277, 'learning_rate': 7.705730715843744e-06, 'epoch': 1.84}\n",
      "{'loss': 0.0021, 'grad_norm': 0.00023118971148505807, 'learning_rate': 7.309141384096769e-06, 'epoch': 1.9}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 26\u001b[0m\n\u001b[0;32m     18\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     19\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     20\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m     21\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_train,\n\u001b[0;32m     22\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtokenized_test,\n\u001b[0;32m     23\u001b[0m )\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Entrenar el modelo\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\samue\\anaconda3\\envs\\project_g\\lib\\site-packages\\transformers\\trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\samue\\anaconda3\\envs\\project_g\\lib\\site-packages\\transformers\\trainer.py:2279\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2278\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 2279\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2282\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2283\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2284\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2285\u001b[0m ):\n\u001b[0;32m   2286\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2287\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\samue\\anaconda3\\envs\\project_g\\lib\\site-packages\\transformers\\trainer.py:3349\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3347\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   3348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3349\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[1;32mc:\\Users\\samue\\anaconda3\\envs\\project_g\\lib\\site-packages\\accelerate\\accelerator.py:2196\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[0;32m   2195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2196\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\samue\\anaconda3\\envs\\project_g\\lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\samue\\anaconda3\\envs\\project_g\\lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\samue\\anaconda3\\envs\\project_g\\lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# Cargar el modelo para clasificaci√≥n de secuencias con 2 etiquetas (Dem√≥crata/Republicano)\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "\n",
    "# Configurar los argumentos de entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# Definir el Trainer con los datos de entrenamiento, que ya incluyen las etiquetas\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    ")\n",
    "\n",
    "# Entrenar el modelo\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9c957871cec4a858302379d4963d652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 300.9307, 'eval_samples_per_second': 55.857, 'eval_steps_per_second': 6.985, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo\n",
    "results = trainer.evaluate()\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el modelo entrenado\n",
    "trainer.save_model(r\"C:\\Users\\samue\\OneDrive\\Escritorio\\Docs\\4GeeksAcademy\\FINAL_PROJECT\\4geeks_finalproject\\src/modelo_entrenado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForSequenceClassification\n",
    "\n",
    "# Cargar el modelo entrenado\n",
    "trainer = RobertaForSequenceClassification.from_pretrained(r\"C:\\Users\\samue\\OneDrive\\Escritorio\\Docs\\4GeeksAcademy\\FINAL_PROJECT\\4geeks_finalproject\\src\\modelo_entrenado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar inferencia sobre el conjunto de test\n",
    "predictions = trainer.predict(tokenized_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'predictions' devuelve un objeto con logits (predicciones sin procesar)\n",
    "logits = predictions.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "# Convertir los logits en probabilidades\n",
    "import numpy as np\n",
    "\n",
    "# Aplicar softmax para convertir los logits en probabilidades\n",
    "probabilities = softmax(logits, axis=1)\n",
    "\n",
    "# Obtener las predicciones finales (clase con mayor probabilidad)\n",
    "predicted_labels = np.argmax(probabilities, axis=1)\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9988696531619965\n",
      "Precision: 0.9982587497823437\n",
      "Recall: 0.9984326018808778\n",
      "F1-score: 0.9983456682629517\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Convertir el conjunto tokenizado a DataFrame para obtener las etiquetas reales\n",
    "test_df = tokenized_test.to_pandas()\n",
    "\n",
    "# Calcular la precisi√≥n (accuracy)\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "# Calcular precision, recall y f1-score\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='binary')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Mover el modelo a la GPU si est√° disponible\u001b[39;00m\n\u001b[0;32m      6\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Texto nuevo para probar\u001b[39;00m\n\u001b[0;32m     10\u001b[0m new_text \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhat a dangerous with all this guns\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRed is the best\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from scipy.special import softmax\n",
    "import numpy as np\n",
    "\n",
    "# Mover el modelo a la GPU si est√° disponible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Texto nuevo para probar\n",
    "new_text = ['what a dangerous with all this guns', \"Red is the best\"]\n",
    "\n",
    "# Tokenizar el texto\n",
    "inputs = tokenizer(new_text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Mover los tensores a la GPU\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "# Realizar inferencia en el nuevo texto\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Mover logits a la CPU para aplicar softmax\n",
    "probabilities = softmax(logits.cpu().numpy(), axis=1)\n",
    "\n",
    "# Obtener las predicciones finales (clase con mayor probabilidad)\n",
    "predicted_labels = np.argmax(probabilities, axis=1)\n",
    "\n",
    "# Mapear las etiquetas predichas a sus nombres (0: Republicano, 1: Dem√≥crata)\n",
    "label_map = {0: \"Republicano\", 1: \"Dem√≥crata\"}\n",
    "predicted_names = [label_map[label] for label in predicted_labels]\n",
    "\n",
    "# Imprimir las predicciones\n",
    "print(predicted_names)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finalproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

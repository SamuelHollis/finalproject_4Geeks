{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "import os\n",
    "import praw\n",
    "import pandas as pd\n",
    "import re\n",
    "from prawcore.exceptions import TooManyRequests, RequestException\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "\n",
    "# filtros trump\n",
    "trump_keywords = [\n",
    "    'donald trump', 'trump', 'donald', 'donaldtrump', \n",
    "    'orange', 'duck', 'donaldduck', 'donald duck', \n",
    "    'donnybaby', 'donnyboy', 'donnybrook', 'trumpinator', \n",
    "    'trumpamaniac', 'trumpastrophie', 'trumpocalypse', \n",
    "    'trumpenstein', 'trumpletoes', 'tricky trump', \n",
    "    'pumpkin', 'corn', 'president trump', \n",
    "    'drumpf', 'the donald', 'mr. trump', 'potus'\n",
    "]\n",
    "trump_keywords = [keyword.lower() for keyword in trump_keywords]\n",
    "\n",
    "# filtros trumpers\n",
    "trumpers_keywords = [\n",
    "    'mike pence', 'pence', 'mike', 'mikepence',\n",
    "    'ron desantis', 'desantis', 'ron', 'rondesantis',\n",
    "    'marjorie taylor greene', 'mtg', 'greene', 'marjorie',\n",
    "    'lauren boebert', 'boebert', 'lauren',\n",
    "    'ted cruz', 'cruz', 'ted', 'tedcruz',\n",
    "    'mitch mcconnell', 'mcconnell', 'mitch', 'mcconnell',\n",
    "    'lindsey graham', 'graham', 'lindsey', 'lindseygraham',\n",
    "    'kevin mccarthy', 'mccarthy', 'kevin', 'kevinmccarthy',\n",
    "    'rudy giuliani', 'giuliani', 'rudy', 'rudygiuliani',\n",
    "    'steve bannon', 'bannon', 'steve', 'stevebannon',\n",
    "    'michael flynn', 'flynn', 'michael', 'michaelflynn',\n",
    "    'roger stone', 'roger', 'stone', 'rogerstone',\n",
    "    'matt gaetz', 'gaetz', 'matt', 'mattgaetz',\n",
    "    'sean hannity', 'hannity', 'sean', 'seanhannity',\n",
    "]\n",
    "trumpers_keywords = [keyword.lower() for keyword in trumpers_keywords]\n",
    "\n",
    "# filtros kamala\n",
    "kamala_keywords = [\n",
    "    'kamala harris', 'kamala', 'harris', 'kamalaharris', \n",
    "    'vice president harris', 'vp harris', 'kammie', \n",
    "    'kammy', 'kamalalal', 'kamalita', 'mrs. harris', 'ms. harris',\n",
    "    'Comrade Kamala', 'Crazy Kamala', 'Laffin Kamala', 'Lying Kamala Harris', 'Kamabla'\n",
    "]\n",
    "kamala_keywords = [keyword.lower() for keyword in kamala_keywords]\n",
    "\n",
    "# filtros kamalers\n",
    "kamalers_keywords = [\n",
    "    'nancy pelosi', 'pelosi', 'nancy', 'nancypelosi',\n",
    "    'chuck schumer', 'schumer', 'chuck', 'chuckschumer',\n",
    "    'elizabeth warren', 'warren', 'elizabeth', 'elizabethwarren',\n",
    "    'bernie sanders', 'sanders', 'bernie', 'berniesanders',\n",
    "    'aoc', 'alexandria ocasio-cortez', 'ocasio-cortez', 'alexandria',\n",
    "    'pete buttigieg', 'buttigieg', 'pete', 'petebuttigieg',\n",
    "    'gavin newsom', 'newsom', 'gavin', 'gavinnewsom',\n",
    "    'cory booker', 'booker', 'cory', 'corybooker',\n",
    "    'stacey abrams', 'abrams', 'stacey', 'staceyabrams',\n",
    "    'hillary', 'michelle obama', 'michelle', 'michelleobama',\n",
    "    'keisha lance bottoms', 'keisha', 'lance bottoms', 'keisha lance'\n",
    "]\n",
    "kamalers_keywords = [keyword.lower() for keyword in kamalers_keywords]\n",
    "\n",
    "# Logging configuration\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s', \n",
    "                    handlers=[logging.StreamHandler(), \n",
    "                              logging.FileHandler(r\"C:\\Users\\34616\\Documents\\4GEEKS\\datos_gordos\\reddit\\Scraping_results\\all_hotscraperv2.log\", mode='a')])\n",
    "\n",
    "# Initializing Reddit API\n",
    "load_dotenv()\n",
    "user_agent = \"political opinion 2.0 by /u/speedylean\"\n",
    "reddit = praw.Reddit(client_id=os.getenv('REDDIT_ID'),\n",
    "                     client_secret=os.getenv('REDDIT_SECRET'),\n",
    "                     user_agent=user_agent)\n",
    "\n",
    "# Functions to check for relevant keywords\n",
    "def contains_trump_keyword(text):\n",
    "    return any(keyword in text.lower() for keyword in trump_keywords)\n",
    "\n",
    "def contains_trumper_keyword(text):\n",
    "    return any(keyword in text.lower() for keyword in trumpers_keywords)\n",
    "\n",
    "def contains_kamala_keyword(text):\n",
    "    return any(keyword in text.lower() for keyword in kamala_keywords)\n",
    "\n",
    "def contains_kamaler_keyword(text):\n",
    "    return any(keyword in text.lower() for keyword in kamalers_keywords)\n",
    "\n",
    "def is_relevant_content(text):\n",
    "    text = text.lower()\n",
    "    bot_phrases = [\n",
    "        \"i am a bot\", \"this bot\", \"automoderator\", \n",
    "        \"bot created\", \"beep boop\", \"bleep bloop\", \n",
    "        \"bot detected\", \"this is a reminder from the bots\",\n",
    "        \"your post has been removed\", \"your comment has been removed\", \n",
    "        \"moderator action\", \"subreddit rules\", \n",
    "        \"thank you for your submission\", \"follow the subreddit rules\", \n",
    "        \"please read our community guidelines\", \"crosspost\", \"x-post\", \"r/\"\n",
    "    ]\n",
    "\n",
    "    if any(phrase in text for phrase in bot_phrases):\n",
    "        return False\n",
    "\n",
    "    if re.search(r'http[s]?://', text):\n",
    "        return False\n",
    "\n",
    "    if len(text) < 20 or text in ['thanks', 'lol', 'ok', 'i agree']:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "# Subreddits to scrape\n",
    "subreddits = ['politics', 'PoliticalDiscussion', 'Conservative', 'Liberal', 'ModeratePolitics', 'Ask_Politics', 'Democrats', 'Republican']\n",
    "\n",
    "# DataFrame columns\n",
    "columns = ['text', 'submission_type', 'subreddit', 'label']\n",
    "\n",
    "# Rate limiting backoff\n",
    "initial_backoff = 5\n",
    "max_backoff = 300\n",
    "backoff_factor = 2\n",
    "\n",
    "# Output file with timestamp\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_file = rf\"C:\\Users\\34616\\Documents\\4GEEKS\\datos_gordos\\reddit\\Scraping_results\\all_hotscrape_v2p10000_{timestamp}.csv\"\n",
    "\n",
    "def save_progress(df_buffer):\n",
    "    try:\n",
    "        if not df_buffer.empty:\n",
    "            logging.info(\"Saving current progress...\")\n",
    "            file_exists = os.path.isfile(output_file)\n",
    "            df_buffer.to_csv(output_file, mode='a', header=not file_exists, index=False)\n",
    "            logging.info(f\"Saved {len(df_buffer)} records successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving progress: {e}\")\n",
    "\n",
    "def backoff_sleep(attempt):\n",
    "    sleep_time = min(initial_backoff * (backoff_factor ** attempt), max_backoff)\n",
    "    logging.info(f\"Rate limit hit. Sleeping for {sleep_time} seconds...\")\n",
    "    time.sleep(sleep_time)\n",
    "\n",
    "def scrape_subreddit(subreddit_name, limit=10000):\n",
    "    records_collected = 0\n",
    "    save_threshold = 1000\n",
    "    buffer = []\n",
    "\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    logging.info(f\"Now scraping r/{subreddit_name}\")\n",
    "\n",
    "    # Fetching posts with error handling\n",
    "    attempt = 0\n",
    "    while True:\n",
    "        try:\n",
    "            posts = list(subreddit.hot(limit=limit))\n",
    "            logging.info(f\"Fetched {len(posts)} posts from r/{subreddit_name}\")\n",
    "            break\n",
    "        except TooManyRequests as e:\n",
    "            logging.warning(f\"Rate limit hit when fetching posts from r/{subreddit_name}: {e}\")\n",
    "            backoff_sleep(attempt)\n",
    "            attempt += 1\n",
    "        except RequestException as e:\n",
    "            logging.error(f\"Request exception when fetching posts from r/{subreddit_name}: {e}\")\n",
    "            backoff_sleep(attempt)\n",
    "            attempt += 1\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unhandled exception when fetching posts from r/{subreddit_name}: {e}\")\n",
    "            break\n",
    "\n",
    "    for post in posts:\n",
    "        attempt = 0\n",
    "        while True:\n",
    "            try:\n",
    "                post.comments.replace_more(limit=None)\n",
    "                comments = post.comments.list()\n",
    "                break\n",
    "            except TooManyRequests as e:\n",
    "                logging.warning(f\"Rate limit hit when fetching comments: {e}\")\n",
    "                backoff_sleep(attempt)\n",
    "                attempt += 1\n",
    "            except RequestException as e:\n",
    "                logging.error(f\"Request exception when fetching comments: {e}\")\n",
    "                backoff_sleep(attempt)\n",
    "                attempt += 1\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Unhandled exception when fetching comments: {e}\")\n",
    "                break\n",
    "        \n",
    "        # Processing comments\n",
    "        for comment in comments:\n",
    "            if not is_relevant_content(comment.body):\n",
    "                continue\n",
    "\n",
    "            label = ''\n",
    "            if contains_trump_keyword(comment.body):\n",
    "                label = 'Trump'\n",
    "            elif contains_trumper_keyword(comment.body):\n",
    "                label = 'Trumpers'\n",
    "            elif contains_kamala_keyword(comment.body):\n",
    "                label = 'Kamala'\n",
    "            elif contains_kamaler_keyword(comment.body):\n",
    "                label = 'Kamalers'\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            record = {\n",
    "                'text': comment.body,\n",
    "                'submission_type': 'comment',\n",
    "                'subreddit': subreddit_name,\n",
    "                'label': label\n",
    "            }\n",
    "\n",
    "            buffer.append(record)\n",
    "            records_collected += 1\n",
    "\n",
    "            # Save progress every save_threshold records\n",
    "            if records_collected >= save_threshold:\n",
    "                df_buffer = pd.DataFrame(buffer, columns=columns)\n",
    "                save_progress(df_buffer)\n",
    "                buffer = []\n",
    "                records_collected = 0\n",
    "\n",
    "    # Save remaining records\n",
    "    if buffer:\n",
    "        df_buffer = pd.DataFrame(buffer, columns=columns)\n",
    "        save_progress(df_buffer)\n",
    "\n",
    "def main(subreddits):\n",
    "    for subreddit in subreddits:\n",
    "        scrape_subreddit(subreddit, limit=10000)\n",
    "        logging.info(f\"Finished scraping r/{subreddit}\")\n",
    "\n",
    "    logging.info(\"Scraping complete.\")\n",
    "\n",
    "\n",
    "main(subreddits)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU_Chiclanera",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

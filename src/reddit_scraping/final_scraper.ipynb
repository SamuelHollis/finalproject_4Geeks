{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "import praw\n",
    "import re\n",
    "from prawcore.exceptions import TooManyRequests, RequestException\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "democrat_keywords = [\n",
    "    # General Democrat references\n",
    "    'democrat', 'dems', 'liberal', 'libs', 'left-wing', 'leftist', 'snowflake', \n",
    "    'blue wave', 'democratic party', 'progressive', 'woke', 'sjw', 'social justice warrior',\n",
    "\n",
    "    # Joe Biden\n",
    "    'biden', 'joe biden', 'sleepy joe', 'creepy joe', 'joey', 'the big guy', 'potato', \n",
    "    'senile joe', 'bidenflation', 'brandon', 'lets go brandon', 'bribem', \n",
    "\n",
    "    # Kamala Harris\n",
    "    'kamala', 'harris', 'kamala harris', 'heels up', 'heels up harris', 'knee pads', 'cameltoe harris', \n",
    "    'kamal', 'kamalala', 'mala mala', 'cackling harris', 'veep harris', 'madam vp', 'vp harris',\n",
    "\n",
    "    # Nancy Pelosi\n",
    "    'pelosi', 'nancy pelosi', 'crazy nancy', 'auntie nancy', 'nancypants', 'nasty nancy', 'pelo-clown', \n",
    "\n",
    "    # Barack Obama\n",
    "    'obama', 'barack obama', 'obummer', 'nobama', 'barry', 'barry soetoro',\n",
    "\n",
    "    # Alexandria Ocasio-Cortez\n",
    "    'aoc', 'alexandria ocasio-cortez', 'ocrazio', 'aocrazio', 'ocasiotard', 'aocloon', 'aoc clown', 'green new deal girl', \n",
    "    'sandy', 'bartender',\n",
    "\n",
    "    # Bernie Sanders\n",
    "    'bernie', 'bernie sanders', 'comrade sanders', 'bernout', 'feel the bern', 'grandpa socialism', 'old bernie', 'crazy bernie',\n",
    "\n",
    "    # Elizabeth Warren\n",
    "    'elizabeth warren', 'warren', 'pocahontas', 'fauxcahontas', 'lizzie warren', 'warren the warrior', 'chief warren',\n",
    "\n",
    "    # Other prominent Democrats\n",
    "    'hillary', 'hillary clinton', 'crooked hillary', 'killary', 'clinton', 'chelsea clinton', 'the clintons', \n",
    "    'adam schiff', 'schifty schiff', 'shifty schiff', 'jerry nadler', 'fat jerry',\n",
    "\n",
    "    # Voters/supporters\n",
    "    'dem voter', 'democrats supporter', 'lib voter', 'woke mob', 'sjw army', 'antifa', 'lefty', 'blm', 'black lives matter', \n",
    "    'democrat loyalist', 'progressive left', 'radical left', 'anarchist', 'feminazi', 'the squad'\n",
    "]\n",
    "\n",
    "democrat_keywords = [keyword.lower() for keyword in democrat_keywords]\n",
    "\n",
    "republican_keywords = [\n",
    "    # General Republican references\n",
    "    'republican', 'gop', 'right-wing', 'rightie', 'conservative', 'maga', 'red wave', 'republican party', \n",
    "    'patriot', 'nationalist', 'the right', 'alt-right', 'alt right', 'chud', 'repubtard', 'repugs', \n",
    "\n",
    "    # Donald Trump\n",
    "    'trump', 'donald trump', 'donald', 'the donald', 'orange man', 'orangutan', 'orangeman bad', 'drumpf', 'trumpster', \n",
    "    'trumptard', 'trumpkin', 'the cheeto', 'cheeto jesus', 'maga king', 'god emperor', 'tangerine tyrant', '45', \n",
    "    'donald dump', 'trumpanzee', 'trumplethinskin', 'trumpenstein', 'orange'\n",
    "\n",
    "    # Other Trump family members\n",
    "    'melania', 'melania trump', 'ivanka', 'ivanka trump', 'eric trump', 'donald jr', 'don jr', 'tiffany trump', 'barron trump',\n",
    "\n",
    "    # Ted Cruz\n",
    "    'ted cruz', 'lyin ted', 'cruz missile', 'creepy cruz', 'texas senator', \n",
    "\n",
    "    # Mitch McConnell\n",
    "    'mitch mcconnell', 'mcconnell', 'mitch the turtle', 'mitch', 'moscow mitch', 'the turtle',\n",
    "\n",
    "    # Ron DeSantis\n",
    "    'ron desantis', 'desantis', 'deathsantis', 'florida man', 'governor desantis', 'ron the con', 'rondesantis', 'rondan', \n",
    "\n",
    "    # Lindsey Graham\n",
    "    'lindsey graham', 'lady g', 'graham cracker', 'closet graham', 'senator graham', 'miss lindsey', \n",
    "\n",
    "    # Other prominent Republicans\n",
    "    'mike pence', 'pence', 'iron mike', 'deputy dog', 'kevin mccarthy', 'sean hannity', 'tucker carlson', \n",
    "    'matt gaetz', 'gaetz', 'fox news', 'gutfeld', 'tom cotton', 'josh hawley', 'lauren boebert', 'marjorie taylor greene', \n",
    "    'mgt', 'green new deal reject',\n",
    "\n",
    "    # Voters/supporters\n",
    "    'maga voter', 'maga mob', 'trump voter', 'trumpist', 'patriot', 'republican loyalist', 'trump supporter', \n",
    "    'deplorable', 'qanon', 'trump army', 'right-winger', 'christian conservative', 'militia', 'gun rights', '2nd amendment'\n",
    "]\n",
    "\n",
    "republican_keywords = [keyword.lower() for keyword in republican_keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Logging configuration\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s', \n",
    "                    handlers=[logging.StreamHandler(), \n",
    "                              logging.FileHandler(r\"C:\\Users\\34616\\Documents\\4GEEKS\\datos_gordos\\reddit\\Scraping_results\\final_scraper.log\", mode='a')])\n",
    "\n",
    "\n",
    "# Functions to check for relevant keywords\n",
    "def contains_keywords(text, keyword_list):\n",
    "    for keyword in keyword_list:\n",
    "        # si la keyword es una frase, buscar la frase entera\n",
    "        if ' ' in keyword:\n",
    "            if keyword.lower() in text.lower():\n",
    "                return True\n",
    "        else:\n",
    "            # si la keyword es una palabra, buscamos Ãºnicamente la palabra entera (no substrings) \n",
    "            if re.search(r'\\b' + re.escape(keyword) + r'\\b', text.lower()):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def contains_republican_keyword(text):\n",
    "    return contains_keywords(text, republican_keywords)\n",
    "\n",
    "def contains_democrat_keyword(text):\n",
    "    return contains_keywords(text, democrat_keywords)\n",
    "\n",
    "def is_relevant_content(text):\n",
    "    text = text.lower()\n",
    "    bot_phrases = [\n",
    "        \"i am a bot\", \"this bot\", \"automoderator\", \n",
    "        \"bot created\", \"beep boop\", \"bleep bloop\", \n",
    "        \"bot detected\", \"this is a reminder from the bots\",\n",
    "        \"your post has been removed\", \"your comment has been removed\", \n",
    "        \"moderator action\", \"subreddit rules\", \n",
    "        \"thank you for your submission\", \"follow the subreddit rules\", \n",
    "        \"please read our community guidelines\", \"crosspost\", \"x-post\", \"r/\"\n",
    "    ]\n",
    "\n",
    "    if any(phrase in text for phrase in bot_phrases):\n",
    "        return False\n",
    "\n",
    "    if re.search(r'http[s]?://', text):\n",
    "        return False\n",
    "\n",
    "    if len(text) < 20 or text in ['thanks', 'lol', 'ok', 'i agree']:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def relabel_posts(df):\n",
    "    new_labels = []\n",
    "\n",
    "    for text in df['text']:\n",
    "        democrat_found = contains_democrat_keyword(text)\n",
    "        republican_found = contains_republican_keyword(text)\n",
    "\n",
    "        if democrat_found and republican_found:\n",
    "            new_labels.append('Both')\n",
    "        elif democrat_found:\n",
    "            new_labels.append('Democrat')\n",
    "        elif republican_found:\n",
    "            new_labels.append('Republican')\n",
    "        else:\n",
    "            new_labels.append('Neutral')\n",
    "    \n",
    "    # Update the DataFrame with the new labels\n",
    "    df['labels'] = new_labels\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subreddits to scrape\n",
    "subreddits = ['politics', 'PoliticalDiscussion', 'Conservative', 'Liberal', 'ModeratePolitics', 'Ask_Politics', 'Democrats', 'Republican']\n",
    "\n",
    "# DataFrame columns\n",
    "columns = ['text', 'submission_type', 'subreddit', 'label']\n",
    "\n",
    "# Rate limiting backoff\n",
    "initial_backoff = 5\n",
    "max_backoff = 300\n",
    "backoff_factor = 2\n",
    "\n",
    "\n",
    "\n",
    "output_file = r\"C:\\Users\\34616\\Documents\\4GEEKS\\datos_gordos\\reddit\\Scraping_results\\final_scraping_result.csv\"\n",
    "\n",
    "def save_progress(df_buffer):\n",
    "    try:\n",
    "        if not df_buffer.empty:\n",
    "            logging.info(\"Saving current progress...\")\n",
    "            file_exists = os.path.isfile(output_file)\n",
    "            df_buffer.to_csv(output_file, mode='a', header=not file_exists, index=False)\n",
    "            logging.info(f\"Saved {len(df_buffer)} records successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving progress: {e}\")\n",
    "\n",
    "def backoff_sleep(attempt):\n",
    "    sleep_time = min(initial_backoff * (backoff_factor ** attempt), max_backoff)\n",
    "    logging.info(f\"Rate limit hit. Sleeping for {sleep_time} seconds...\")\n",
    "    time.sleep(sleep_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing Reddit API\n",
    "load_dotenv()\n",
    "user_agent = \"final scrape /u/speedylean\"\n",
    "reddit = praw.Reddit(client_id=os.getenv('REDDIT_ID'),\n",
    "                     client_secret=os.getenv('REDDIT_SECRET'),\n",
    "                     user_agent=user_agent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def scrape_subreddit(subreddit_name, limit=None):\n",
    "    records_collected = 0\n",
    "    save_threshold = 1000\n",
    "    buffer = []\n",
    "\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    logging.info(f\"Now scraping r/{subreddit_name}\")\n",
    "\n",
    "    # Fetching posts with error handling\n",
    "    attempt = 0\n",
    "    while True:\n",
    "        try:\n",
    "            posts = list(subreddit.hot(limit=limit))\n",
    "            logging.info(f\"Fetched {len(posts)} posts from r/{subreddit_name}\")\n",
    "            break\n",
    "        except TooManyRequests as e:\n",
    "            logging.warning(f\"Rate limit hit when fetching posts from r/{subreddit_name}: {e}\")\n",
    "            backoff_sleep(attempt)\n",
    "            attempt += 1\n",
    "        except RequestException as e:\n",
    "            logging.error(f\"Request exception when fetching posts from r/{subreddit_name}: {e}\")\n",
    "            backoff_sleep(attempt)\n",
    "            attempt += 1\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unhandled exception when fetching posts from r/{subreddit_name}: {e}\")\n",
    "            break\n",
    "\n",
    "    for post in posts:\n",
    "        attempt = 0\n",
    "        while True:\n",
    "            try:\n",
    "                post.comments.replace_more(limit=None)\n",
    "                comments = post.comments.list()\n",
    "                break\n",
    "            except TooManyRequests as e:\n",
    "                logging.warning(f\"Rate limit hit when fetching comments: {e}\")\n",
    "                backoff_sleep(attempt)\n",
    "                attempt += 1\n",
    "            except RequestException as e:\n",
    "                logging.error(f\"Request exception when fetching comments: {e}\")\n",
    "                backoff_sleep(attempt)\n",
    "                attempt += 1\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Unhandled exception when fetching comments: {e}\")\n",
    "                break\n",
    "        \n",
    "        # Processing comments\n",
    "        for comment in comments:\n",
    "            if not is_relevant_content(comment.body):\n",
    "                continue\n",
    "\n",
    "            label = ''\n",
    "            if contains_democrat_keyword(comment.body):\n",
    "                label = 'Democrat'\n",
    "            elif contains_republican_keyword(comment.body):\n",
    "                label = 'Republican'\n",
    "\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            record = {\n",
    "                'text': comment.body,\n",
    "                'submission_type': 'comment',\n",
    "                'subreddit': subreddit_name,\n",
    "                'label': label\n",
    "            }\n",
    "\n",
    "            buffer.append(record)\n",
    "            records_collected += 1\n",
    "\n",
    "            # Save progress every save_threshold records\n",
    "            if records_collected >= save_threshold:\n",
    "                df_buffer = pd.DataFrame(buffer, columns=columns)\n",
    "                save_progress(df_buffer)\n",
    "                buffer = []\n",
    "                records_collected = 0\n",
    "\n",
    "    # Save remaining records\n",
    "    if buffer:\n",
    "        df_buffer = pd.DataFrame(buffer, columns=columns)\n",
    "        save_progress(df_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-12 19:06:08,627 - INFO - Now scraping r/politics\n",
      "2024-09-12 19:06:17,901 - INFO - Fetched 810 posts from r/politics\n"
     ]
    }
   ],
   "source": [
    "def main(subreddits):\n",
    "    for subreddit in subreddits:\n",
    "        scrape_subreddit(subreddit, limit=10000)\n",
    "        logging.info(f\"Finished scraping r/{subreddit}\")\n",
    "\n",
    "    logging.info(\"Scraping complete.\")\n",
    "\n",
    "\n",
    "main(subreddits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_pluja",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
